name: 'Test Glue script'
on:
  workflow_dispatch:

  pull_request:
    branches:
      - 'prod'
      - 'stage'
      - 'test'
      - 'develop'
    
jobs:
  test:

    runs-on: ubuntu-latest
    strategy:
      matrix:
        spark-version: [3.1.1]
        hadoop-version: [3.2]
        py4j-version: [0.10.9]

    steps:
    - name: Checkout1
      uses: actions/checkout@v2

    - name: Set up Python 3.7
      uses: actions/setup-python@v2
      with:
        python-version: 3.7
    - name: Set up JAVA 11
      uses: actions/setup-java@v1
      with:
        java-version: 11

    - name: Set download path
      run: |
        echo installFolder='/opt/hostedtoolcache' >> $GITHUB_ENV

    - name: Spark cache
      uses: actions/cache@v2
      id: spark-cache
      with:
        path: ${{ env.installFolder }}/spark-${{ matrix.spark-version }}-bin-hadoop${{ matrix.hadoop-version }}
        key: ${{ env.installFolder }}-spark-${{ hashFiles('spark') }}

    - name: Download Spark
      if: steps.spark-cache.outputs.cache-hit != 'true'
      run: |
        cd /tmp
        wget -q https://archive.apache.org/dist/spark/spark-${{ matrix.spark-version }}/spark-${{ matrix.spark-version }}-bin-hadoop${{ matrix.hadoop-version }}.tgz
        tar xzf spark-${{ matrix.spark-version }}-bin-hadoop${{ matrix.hadoop-version }}.tgz -C ${installFolder}
        rm spark-${{ matrix.spark-version }}-bin-hadoop${{ matrix.hadoop-version }}.tgz

    - name: Setup Spark
      run: |
        ln -s "${installFolder}/spark-${{ matrix.spark-version }}-bin-hadoop${{ matrix.hadoop-version }}" ${installFolder}/spark
        echo py4j-version=${{ matrix.py4j-version }} >> $GITHUB_ENV
        echo SPARK_HOME=${installFolder}/spark >> $GITHUB_ENV
        echo SPARK_OPTS='--driver-java-options=-Xms1024M --driver-java-options=-Xmx2048M --driver-java-options=-Dlog4j.logLevel=info' >> $GITHUB_ENV
        echo PYSPARK_PYTHON='python' >> $GITHUB_ENV
        echo PYSPARK_DRIVER_PYTHON='python' >> $GITHUB_ENV

    - name: Set path vars
      run: |
        echo PYTHONPATH=${SPARK_HOME}/python:${SPARK_HOME}/python/lib/py4j-${{ matrix.py4j-version }}-src.zip >> $GITHUB_ENV
        echo PATH=$PATH:$SPARK_HOME/bin >> $GITHUB_ENV

    - run: spark-submit --version

    - name: Pip cache
      uses: actions/cache@v2
      id: pip-cache
      with:
        path: ${{ env.pythonLocation }}
        key: ${{ env.pythonLocation }}-${{ hashFiles('requirements_dev.txt') }}

    - name: Install python dependencies
      run: |
        python3 -m pip install --upgrade pip
        pip install -r requirements_dev.txt
        pip install flake8

    - name: Install other dependencies
      if: steps.pip-cache.outputs.cache-hit != 'true'
      run: |
        sudo apt-get install subversion
        svn export https://github.com/awslabs/aws-glue-libs/trunk/awsglue $pythonLocation/lib/python3.7/awsglue

    - name: Lint with flake8
      run: |
        # stop the build if there are Python syntax errors or undefined names
        flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics
        # exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide
        flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics

    - name: Test with pytest, pylint and bandit
      run: |
        export PYTHONPATH=$PYTHONPATH:/home/runner/work/${{ github.event.repository.name }}/${{ github.event.repository.name }}/
        python run_checks.py
